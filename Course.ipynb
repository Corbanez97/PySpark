{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2660c7d1-7406-44f8-a108-c79790551350",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55121bba-eb99-44a1-8337-595e6c485830",
   "metadata": {},
   "source": [
    "Spark is a tool for data processing. It can be used as a data storage, but it is not its main capability.\n",
    "\n",
    "Spark is a great tool for Big Data for its clustering processing method. Data processed by Spark enables partitioning, scalability, etc. Another great feature of this framework is the duplication of data along the cluster. This makes data available even if one node of the cluster fails to continue processing (dies or something XP)\n",
    "\n",
    "Looking further into partitioning, is the process of separating data into particular groups. Groups that has its unique processing executors. This is a great way to reduce reprocessing of data.\n",
    "\n",
    "We will learn PySpark, one possible language of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803309c4-8cf1-4653-ac23-be51031f7da4",
   "metadata": {},
   "source": [
    "## 1.1. Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beae30b-93a0-4878-84e8-b57e3a314a26",
   "metadata": {},
   "source": [
    "* Machine Learning (Mlib)\n",
    "* SQL (Spark SQL)\n",
    "* Streaming processing\n",
    "* Graphs processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcea8a9-e605-48ba-8dd9-4bb7a37cef3a",
   "metadata": {},
   "source": [
    "## 1.2. Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c95a0a-49f4-4723-8027-a875de77d932",
   "metadata": {},
   "source": [
    "* **Driver**: initialize SparkSession and acquire computational resources of the Cluster Manager; transforms operation into DAGs (Directed Acyclic Graphs); distribute operations throughout executors.\n",
    "\n",
    "* **Manager**: manage cluster's resorces. We have four different managers, built-in, YARN, Mesos and Kubernetes.\n",
    "\n",
    "* **Executer**: Runs throughout each node executing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84bb31-2d06-449b-a37d-f61c1f3d60fb",
   "metadata": {},
   "source": [
    "## 1.3. Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfc620-fd96-46da-a2ec-19209efc1144",
   "metadata": {},
   "source": [
    "The dataframe is the basic unit of Spark. They are immutable, characteristic that bring failed tolerance.\n",
    "When we execute process in Spark we have two basic operation: Transformations and Actions.\n",
    "\n",
    "Transformations generate a new df. And, the processing of a transformation only occurs once an Action happens (Lazy Evaluation),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65af69-0669-4a86-b052-bda335264432",
   "metadata": {},
   "source": [
    "![Lazy Evaluation](images/Lazy.png \"Lazy Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c6f31-6979-4c38-88b7-65899d76c7f5",
   "metadata": {},
   "source": [
    "We have two main types of transformations: Narrow and Wide. they indicate if the transform uses data from the same (Narrow) or different (Wide) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71360845-92f3-4a1f-826a-9c5680f29269",
   "metadata": {},
   "source": [
    "## 1.4. Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1cb06-252f-4288-be3f-ad216287beb2",
   "metadata": {},
   "source": [
    "* Job\n",
    "* Stage\n",
    "* Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d52f2-3de5-4b7a-8859-53fa87976494",
   "metadata": {},
   "source": [
    "![Spark Components](images/Contents.png \"Spark Components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827fe6d-8a99-4ac3-b325-1c08d596ad1f",
   "metadata": {},
   "source": [
    "## 1.5. Big Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ff47e-62e8-4172-8c4c-719acb16c204",
   "metadata": {},
   "source": [
    "Modern data formats are open to every capable framework to read. Throughout this course, we will use **parquet** files. These data formats are decoupled from the reading tools. They are also binary and compressed files. Moreover, they support schemas, are passive to clustering and partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9021103-1ef5-4c99-be6e-56e1d228cab6",
   "metadata": {},
   "source": [
    "## 1.6. Installation and initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89eaefa-ad9f-4ede-a045-fd38353e862a",
   "metadata": {},
   "source": [
    "To install Spark, one must go to their website and copy the download link. Then, simply copy the link into the terminal with the **wget** command. After this, you must move the extracted folder into the **opt** folder and add the required environmental variables to the **~/.bashrc** file.\n",
    "\n",
    "Once the installation and variables are setup, use the following url on the browser to validate: **http://localhost:8080/**\n",
    "\n",
    "Finnaly, to access Spark through the terminal, run the following commands:\n",
    "* start-master.sh\n",
    "* /opt/spark/sbin/start-slave.sh spark://localhost:7077\n",
    "\n",
    "Now, one can access the Spark shell (python language) via **pyspark** command. For this course, you must also install **numpy** and **pandas**\n",
    "\n",
    "*P.S.: For PySpark only, just use **pip install pyspark** ʕ•ᴥ•ʔ*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0571e-3cfd-4dc5-8049-287fe50fdbb4",
   "metadata": {},
   "source": [
    "![Spark Successfull Install](images/install_success.png \"Spark Successfull Installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97a786-0cf5-4ea8-ae00-aeb686c4c30a",
   "metadata": {},
   "source": [
    "For a more in depth reading, see: https://www.bmc.com/blogs/jupyter-notebooks-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead1514-eab7-4802-bc6a-3ba99fe49c36",
   "metadata": {},
   "source": [
    "# 2. Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865115bb-cbe6-468f-a7c2-509e9e948e8c",
   "metadata": {},
   "source": [
    "The Spark framework can interpret three data structures: **RDD - Resilien Distributed Datasets**; **Datasets**; **DataFrames**.\n",
    "\n",
    "RDD are the most basic structure that Spark can process, and they normaly are:\n",
    "\n",
    "* Low level basic data structure\n",
    "* Complex and wordy - one might need a lot of code to process RDD\n",
    "* Not optimized for Spark\n",
    "\n",
    "DataFrames and Dataset are easier to manipulate. We already know their tabular structure, however, Datasets are not available for PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0836091-00bb-4e8f-8063-0b10290ee5a7",
   "metadata": {},
   "source": [
    "## 2.1. RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1157b6-e194-40e3-8f74-68457890a5eb",
   "metadata": {},
   "source": [
    "One might create an **RDD** on shell by calling the method **sc.parallelize**, which takes as an argument a list.\n",
    "Ex: ***nums = sc.parallelize([1,2,3,4,5,6,7,8,9,10])***\n",
    "\n",
    "Another way to create the said object is via a hardcoded inicialization of a PySpark Session. The next kernel has such code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb725af0-fbcd-4c3f-9d4e-5a4238583415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc20638-09ed-46d6-9817-e06e5d3c053f",
   "metadata": {},
   "source": [
    "This object has a variaety of methods, such as **take**, **top**, **colect**, **sum**, **mean**, **stdev**, etc.\n",
    "\n",
    "Moreover, using python lambda functions one can apply transformations - such as **filter** and **map**- on the **RDD**, then, perform an action to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06bf3ca4-5113-4a13-a32c-5364534c1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take method: [1, 2, 3, 4, 5]\n",
      "Count method: 12\n",
      "Standard deviation method: 3.452052529534663\n",
      "Collecting filtered RDD: [5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Collecting mapped RDD: [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36]\n"
     ]
    }
   ],
   "source": [
    "print(f'Take method: {rdd.take(5)}')\n",
    "\n",
    "print(f'Count method: {rdd.count()}')\n",
    "\n",
    "print(f'Standard deviation method: {rdd.stdev()}')\n",
    "\n",
    "#Filter using lambda functions\n",
    "rdd_filtered = rdd.filter(lambda rdd_filtered: rdd_filtered > 4) \n",
    "print(f'Collecting filtered RDD: {rdd_filtered.collect()}')\n",
    "#Collect method to collect data. Not a great idea for actual bigdata ｡◕‿◕｡\n",
    "\n",
    "#Mapping using lambda functions\n",
    "rdd_mapped = rdd.map(lambda rdd_mapped: rdd_mapped*3)\n",
    "print(f'Collecting mapped RDD: {rdd_mapped.collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642ecee-df1a-4d51-bdb4-e5c14cb0a61c",
   "metadata": {},
   "source": [
    "With two **RDDs**, one can manipulate them with similar methods from mathematical sets. We can perform **union**, **intersection**, **subtract**, **cartesian**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1111b056-64ae-4854-979e-f546ed714213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of both RDDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 10, 11, 12, 13, 14, 15]\n",
      "Intersection of both RDDs: [10, 11, 12]\n",
      "Subtraction of both RDDs: [8, 1, 9, 2, 3, 4, 5, 6, 7]\n",
      "Cartesian product of both RDDs: [(1, 10), (2, 10), (3, 10), (1, 11), (2, 11), (3, 11), (1, 12), (2, 12), (3, 12), (1, 13), (2, 13), (3, 13), (1, 14), (2, 14), (3, 14), (1, 15), (2, 15), (3, 15), (4, 10), (5, 10), (6, 10), (4, 11), (5, 11), (6, 11), (4, 12), (5, 12), (6, 12), (4, 13), (5, 13), (6, 13), (4, 14), (5, 14), (6, 14), (4, 15), (5, 15), (6, 15), (7, 10), (8, 10), (9, 10), (7, 11), (8, 11), (9, 11), (7, 12), (8, 12), (9, 12), (7, 13), (8, 13), (9, 13), (7, 14), (8, 14), (9, 14), (7, 15), (8, 15), (9, 15), (10, 10), (11, 10), (12, 10), (10, 11), (11, 11), (12, 11), (10, 12), (11, 12), (12, 12), (10, 13), (11, 13), (12, 13), (10, 14), (11, 14), (12, 14), (10, 15), (11, 15), (12, 15)]\n"
     ]
    }
   ],
   "source": [
    "data2 = [10, 11, 12, 13, 14, 15]\n",
    "rdd2 = spark.sparkContext.parallelize(data2)\n",
    "\n",
    "union = rdd.union(rdd2)\n",
    "print(f'Union of both RDDs: {union.collect()}')\n",
    "      \n",
    "inter = rdd.intersection(rdd2)\n",
    "print(f'Intersection of both RDDs: {inter.collect()}')\n",
    "\n",
    "sub = rdd.subtract(rdd2)\n",
    "print(f'Subtraction of both RDDs: {sub.collect()}')\n",
    "\n",
    "cartesian_prod = rdd.cartesian(rdd2)\n",
    "print(f'Cartesian product of both RDDs: {cartesian_prod.collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebace57-7daa-4cdf-b03e-43fbf7bf3c9f",
   "metadata": {},
   "source": [
    "Now let us take a look at an example. Say the previous cartesian product is the sales registry of a given store. The first entry of each tuple is the customer code, and the second entry is the number of cucumbers they bought in our store. (◕‿◕✿)\n",
    "\n",
    "We can extract **keys** and **values** from our registry, and create different **RDDs** to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cbdee70-aedc-4dec-b86b-a872513b9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct customers that bought cucumbers on our store: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Total number of cucumber bought: 900.\n",
      "OH YEAH! That's a lot of CUCUMBAS! (づ￣ ³￣)づ\n",
      "Count how many times each customer passed by our store: defaultdict(<class 'int'>, {1: 6, 2: 6, 3: 6, 4: 6, 5: 6, 6: 6, 7: 6, 8: 6, 9: 6, 10: 6, 11: 6, 12: 6})\n"
     ]
    }
   ],
   "source": [
    "cucumbaLTDA_registry = cartesian_prod\n",
    "\n",
    "customers = cucumbaLTDA_registry.keys().distinct()\n",
    "print(f'Distinct customers that bought cucumbers on our store: {customers.collect()}')\n",
    "\n",
    "total_value = cucumbaLTDA_registry.values().sum()\n",
    "print(f'Total number of cucumber bought: {total_value}.\\nOH YEAH! That\\'s a lot of CUCUMBAS! (づ￣ ³￣)づ')\n",
    "\n",
    "#This reincidence count is broken because we've created our registry via a cartesian product (ಥ﹏ಥ)\n",
    "print(f'Count how many times each customer passed by our store: {cucumbaLTDA_registry.countByKey()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f462f5-a3fa-4096-ba67-9c8171b27d30",
   "metadata": {},
   "source": [
    "Let's say that we also have a registry of debts for some customers. We may create a **join RDD**, where we have the registry of purchase while maintaining the number of unpaid cucumbers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8549a5f1-2478-4a74-a382-931d858e515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete registry obtained by the join of our sales registry and our debts is:\n",
      " [(1, (10, 3)), (1, (11, 3)), (1, (12, 3)), (1, (13, 3)), (1, (14, 3)), (1, (15, 3)), (5, (10, 10)), (5, (11, 10)), (5, (12, 10)), (5, (13, 10)), (5, (14, 10)), (5, (15, 10)), (10, (10, 2)), (10, (11, 2)), (10, (12, 2)), (10, (13, 2)), (10, (14, 2)), (10, (15, 2))]\n",
      "Purchases made by clients without debts: [(2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15)]\n"
     ]
    }
   ],
   "source": [
    "debts = [(1,3),(5,10),(10,2)]\n",
    "unpaid_cucumbers = spark.sparkContext.parallelize(debts)\n",
    "\n",
    "comp_registry = cucumbaLTDA_registry.join(unpaid_cucumbers)\n",
    "print(f'The complete registry obtained by the join of our sales registry and our debts is:\\n {comp_registry.collect()}')\n",
    "\n",
    "purchase_by_no_debt = cucumbaLTDA_registry.subtractByKey(unpaid_cucumbers)\n",
    "print(f'Purchases made by clients without debts: {purchase_by_no_debt.collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d3be1-d376-46be-a1f3-96ea739c157b",
   "metadata": {},
   "source": [
    "## 2.2. DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9148cf-229b-412f-83c3-ebe0c39e38ef",
   "metadata": {},
   "source": [
    "**DataFrames** are the data structure that we will focus on in this course. Some bullet points that are of great importance about **DataFrames**:\n",
    "\n",
    "* Tabular structure. Excel-like data ☜(ﾟヮﾟ☜)\n",
    "* Immutable\n",
    "* Possess know schemas\n",
    "* Preserved linage. Transformations are saved step by step\n",
    "* Columns may have different d-types\n",
    "* Common methods such as **group by**, **order by** and **filter**\n",
    "* ***Extremely optimized on Spark***\n",
    "\n",
    "Without further adieu, let's create a **DataFrame** and play with it. As you may see, to correctly create a **DF** one need to pass as arguments the required data and its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bde3aa4e-6c81-4ff1-9afd-a2b26dbdf5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 15|\n",
      "| Mary| 14|\n",
      "|James| 12|\n",
      "+-----+---+\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 15|\n",
      "+----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3 = [('John', 15),('Mary', 14),('James', 12)]\n",
    "\n",
    "schema = \"Name STRING, Age INT\" #Model of a schema accepted by spark\n",
    "\n",
    "df = spark.createDataFrame(data3, schema) #Here we pass the data and its schema\n",
    "\n",
    "df.show()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c62106b-2bd4-4861-ab9b-6885f6c735dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|   Product|Quantity|\n",
      "+----------+--------+\n",
      "|       Pen|       9|\n",
      "|Pineappple|      22|\n",
      "|     Apple|      12|\n",
      "|       Pen|      13|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema2 = \"Product STRING, Quantity INT\"\n",
    "data4 = [('Pen', 9),('Pineappple', 22),('Apple', 12),('Pen', 13)]\n",
    "\n",
    "df2 = spark.createDataFrame(data4, schema2)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc2f53-a049-4e44-9a01-dcb3c676bc59",
   "metadata": {},
   "source": [
    "The new **DF** contains data from purchased products. One may need to see to total sum of a particular product, for instance. With this intention, we will use the method **groupBy** and **agg** (*agregate*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b38ff34f-1c3d-4c9f-b879-0719295549d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|   Product|sum(Quantity)|\n",
      "+----------+-------------+\n",
      "|       Pen|           22|\n",
      "|Pineappple|           22|\n",
      "|     Apple|           12|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df2.groupBy(\"Product\").agg(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccfc47-59b6-45eb-96d7-28a3b8778a6a",
   "metadata": {},
   "source": [
    "We can use the method **select** to choose different columns, or even add a particular expression for a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fefe6a2-bbb0-48d3-b15f-59310ea297d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+----------------+\n",
      "|   Product|Quantity|(Quantity - 4)|(Quantity * 0.5)|\n",
      "+----------+--------+--------------+----------------+\n",
      "|       Pen|       9|             5|             4.5|\n",
      "|Pineappple|      22|            18|            11.0|\n",
      "|     Apple|      12|             8|             6.0|\n",
      "|       Pen|      13|             9|             6.5|\n",
      "+----------+--------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df2.select(\"Product\", \"Quantity\", expr(\"Quantity - 4\"), expr(\"Quantity * 0.5\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0a53f-2a9e-487d-93e2-fdc3a3fb29df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
