{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2660c7d1-7406-44f8-a108-c79790551350",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55121bba-eb99-44a1-8337-595e6c485830",
   "metadata": {},
   "source": [
    "Spark is a tool for data processing. It can be used as a data storage, but it is not its main capability.\n",
    "\n",
    "Spark is a great tool for Big Data for its clustering processing method. Data processed by Spark enables partitioning, scalability, etc. Another great feature of this framework is the duplication of data along the cluster. This makes data available even if one node of the cluster fails to continue processing (dies or something XP)\n",
    "\n",
    "Looking further into partitioning, is the process of separating data into particular groups. Groups that has its unique processing executors. This is a great way to reduce reprocessing of data.\n",
    "\n",
    "We will learn PySpark, one possible language of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803309c4-8cf1-4653-ac23-be51031f7da4",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beae30b-93a0-4878-84e8-b57e3a314a26",
   "metadata": {},
   "source": [
    "* Machine Learning (Mlib)\n",
    "* SQL (Spark SQL)\n",
    "* Streaming processing\n",
    "* Graphs processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcea8a9-e605-48ba-8dd9-4bb7a37cef3a",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c95a0a-49f4-4723-8027-a875de77d932",
   "metadata": {},
   "source": [
    "* **Driver**: initialize SparkSession and acquire computational resources of the Cluster Manager; transforms operation into DAGs (Directed Acyclic Graphs); distribute operations throughout executors.\n",
    "\n",
    "* **Manager**: manage cluster's resorces. We have four different managers, built-in, YARN, Mesos and Kubernetes.\n",
    "\n",
    "* **Executer**: Runs throughout each node executing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84bb31-2d06-449b-a37d-f61c1f3d60fb",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfc620-fd96-46da-a2ec-19209efc1144",
   "metadata": {},
   "source": [
    "The dataframe is the basic unit of Spark. They are immutable, characteristic that bring failed tolerance.\n",
    "When we execute process in Spark we have two basic operation: Transformations and Actions.\n",
    "\n",
    "Transformations generate a new df. And, the processing of a transformation only occurs once an Action happens (Lazy Evaluation),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65af69-0669-4a86-b052-bda335264432",
   "metadata": {},
   "source": [
    "![Lazy Evaluation](images/Lazy.png \"Lazy Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c6f31-6979-4c38-88b7-65899d76c7f5",
   "metadata": {},
   "source": [
    "We have two main types of transformations: Narrow and Wide. they indicate if the transform uses data from the same (Narrow) or different (Wide) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71360845-92f3-4a1f-826a-9c5680f29269",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1cb06-252f-4288-be3f-ad216287beb2",
   "metadata": {},
   "source": [
    "* Job\n",
    "* Stage\n",
    "* Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d52f2-3de5-4b7a-8859-53fa87976494",
   "metadata": {},
   "source": [
    "![Spark Components](images/Contents.png \"Spark Components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827fe6d-8a99-4ac3-b325-1c08d596ad1f",
   "metadata": {},
   "source": [
    "## Big Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ff47e-62e8-4172-8c4c-719acb16c204",
   "metadata": {},
   "source": [
    "Modern data formats are open to every capable framework to read. Throughout this course, we will use **parquet** files. These data formats are decoupled from the reading tools. They are also binary and compressed files. Moreover, they support schemas, are passive to clustering and partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9021103-1ef5-4c99-be6e-56e1d228cab6",
   "metadata": {},
   "source": [
    "## Installation and initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89eaefa-ad9f-4ede-a045-fd38353e862a",
   "metadata": {},
   "source": [
    "To install Spark, one must go to their website and copy the download link. Then, simply copy the link into the terminal with the **wget** command. After this, you must move the extracted folder into the **opt** folder and add the required environmental variables to the **~/.bashrc** file.\n",
    "\n",
    "Once the installation and variables are setup, use the following url on the browser to validate: **http://localhost:8080/**\n",
    "\n",
    "Finnaly, to access Spark through the terminal, run the following commands:\n",
    "* start-master.sh\n",
    "* /opt/spark/sbin/start-slave.sh spark://localhost:7077\n",
    "\n",
    "Now, one can access the Spark shell (python language) via **pyspark** command. For this course, you must also install **numpy** and **pandas**\n",
    "\n",
    "*P.S.: For PySpark only, just use **pip install pyspark** ʕ•ᴥ•ʔ*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0571e-3cfd-4dc5-8049-287fe50fdbb4",
   "metadata": {},
   "source": [
    "![Spark Successfull Install](images/install_success.png \"Spark Successfull Installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97a786-0cf5-4ea8-ae00-aeb686c4c30a",
   "metadata": {},
   "source": [
    "For a more in depth reading, see: https://www.bmc.com/blogs/jupyter-notebooks-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead1514-eab7-4802-bc6a-3ba99fe49c36",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865115bb-cbe6-468f-a7c2-509e9e948e8c",
   "metadata": {},
   "source": [
    "The Spark framework can interpret three data structures: **RDD - Resilien Distributed Datasets**; **Datasets**; **DataFrames**.\n",
    "\n",
    "RDD are the most basic structure that Spark can process, and they normaly are:\n",
    "\n",
    "* Low level basic data structure\n",
    "* Complex and wordy - one might need a lot of code to process RDD\n",
    "* Not optimized for Spark\n",
    "\n",
    "DataFrames and Dataset are easier to manipulate. We already know their tabular structure, however, Datasets are not available for PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0836091-00bb-4e8f-8063-0b10290ee5a7",
   "metadata": {},
   "source": [
    "## RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1157b6-e194-40e3-8f74-68457890a5eb",
   "metadata": {},
   "source": [
    "One might create an **RDD** on shell by calling the method **sc.parallelize**, which takes as an argument a list.\n",
    "Ex: ***nums = sc.parallelize([1,2,3,4,5,6,7,8,9,10])***\n",
    "\n",
    "Another way to create the said object is via a hardcoded inicialization of a PySpark Session. The next kernel has such code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb725af0-fbcd-4c3f-9d4e-5a4238583415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/22 18:27:20 WARN Utils: Your hostname, corbanez-H110M-H resolves to a loopback address: 127.0.1.1; using 192.168.0.131 instead (on interface enp2s0)\n",
      "22/03/22 18:27:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/22 18:27:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc20638-09ed-46d6-9817-e06e5d3c053f",
   "metadata": {},
   "source": [
    "This object has a variaety of methods, such as **take**, **top**, **colect**, **sum**, **mean**, **stdev**, etc.\n",
    "\n",
    "Moreover, using python lambda functions one can apply transformations - such as **filter** and **map**- on the **RDD**, then, perform an action to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06bf3ca4-5113-4a13-a32c-5364534c1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take method: [1, 2, 3, 4, 5]\n",
      "Count method: 12\n",
      "Standard deviation method: 3.452052529534663\n",
      "Collecting filtered RDD: [5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Collecting mapped RDD: [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36]\n"
     ]
    }
   ],
   "source": [
    "print(f'Take method: {rdd.take(5)}')\n",
    "\n",
    "print(f'Count method: {rdd.count()}')\n",
    "\n",
    "print(f'Standard deviation method: {rdd.stdev()}')\n",
    "\n",
    "#Filter using lambda functions\n",
    "rdd_filtered = rdd.filter(lambda rdd_filtered: rdd_filtered > 4) \n",
    "print(f'Collecting filtered RDD: {rdd_filtered.collect()}')\n",
    "#Collect method to collect data. Not a great idea for actual bigdata ｡◕‿◕｡\n",
    "\n",
    "#Mapping using lambda functions\n",
    "rdd_mapped = rdd.map(lambda rdd_mapped: rdd_mapped*3)\n",
    "print(f'Collecting mapped RDD: {rdd_mapped.collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642ecee-df1a-4d51-bdb4-e5c14cb0a61c",
   "metadata": {},
   "source": [
    "With two **RDDs**, one can manipulate them with similar methods from mathematical sets. We can perform **union**, **intersection**, **subtract**, **cartesian**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1111b056-64ae-4854-979e-f546ed714213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of both RDDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 10, 11, 12, 13, 14, 15]\n",
      "Intersection of both RDDs: [10, 12, 11]\n",
      "Subtraction of both RDDs: [2, 4, 6, 8, 1, 3, 5, 7, 9]\n",
      "Cartesian product of both RDDs: [(1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15)]\n"
     ]
    }
   ],
   "source": [
    "data2 = [10, 11, 12, 13, 14, 15]\n",
    "rdd2 = spark.sparkContext.parallelize(data2)\n",
    "\n",
    "union = rdd.union(rdd2)\n",
    "print(f'Union of both RDDs: {union.collect()}')\n",
    "      \n",
    "inter = rdd.intersection(rdd2)\n",
    "print(f'Intersection of both RDDs: {inter.collect()}')\n",
    "\n",
    "sub = rdd.subtract(rdd2)\n",
    "print(f'Subtraction of both RDDs: {sub.collect()}')\n",
    "\n",
    "cartesian_prod = rdd.cartesian(rdd2)\n",
    "print(f'Cartesian product of both RDDs: {cartesian_prod.collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebace57-7daa-4cdf-b03e-43fbf7bf3c9f",
   "metadata": {},
   "source": [
    "Now let us take a look at an example. Say the previous cartesian product is the sales registry of a given store. The first entry of each tuple is the customer code, and the second entry is the number of cucumbers they bought in our store. (◕‿◕✿)\n",
    "\n",
    "We can extract **keys** and **values** from our registry, and create different **RDDs** to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbdee70-aedc-4dec-b86b-a872513b9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct customers that bought cucumbers on our store: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Total number of cucumber bought: 900.\n",
      "OH YEAH! That's a lot of CUCUMBAS! (づ￣ ³￣)づ\n",
      "Count how many times each customer passed by our store: defaultdict(<class 'int'>, {1: 6, 2: 6, 3: 6, 4: 6, 5: 6, 6: 6, 7: 6, 8: 6, 9: 6, 10: 6, 11: 6, 12: 6})\n"
     ]
    }
   ],
   "source": [
    "cucumbaLTDA_registry = cartesian_prod\n",
    "\n",
    "customers = cucumbaLTDA_registry.keys().distinct()\n",
    "print(f'Distinct customers that bought cucumbers on our store: {customers.collect()}')\n",
    "\n",
    "total_value = cucumbaLTDA_registry.values().sum()\n",
    "print(f'Total number of cucumber bought: {total_value}.\\nOH YEAH! That\\'s a lot of CUCUMBAS! (づ￣ ³￣)づ')\n",
    "\n",
    "#This reincidence count is broken because we've created our registry via a cartesian product (ಥ﹏ಥ)\n",
    "print(f'Count how many times each customer passed by our store: {cucumbaLTDA_registry.countByKey()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f462f5-a3fa-4096-ba67-9c8171b27d30",
   "metadata": {},
   "source": [
    "Let's say that we also have a registry of debts for some customers. We may create a **join RDD**, where we have the registry of purchase while maintaining the number of unpaid cucumbers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8549a5f1-2478-4a74-a382-931d858e515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete registry obtained by the join of our sales registry and our debts is:\n",
      " [(10, (10, 2)), (10, (11, 2)), (10, (12, 2)), (10, (13, 2)), (10, (14, 2)), (10, (15, 2)), (1, (10, 3)), (1, (11, 3)), (1, (12, 3)), (1, (13, 3)), (1, (14, 3)), (1, (15, 3)), (5, (10, 10)), (5, (11, 10)), (5, (12, 10)), (5, (13, 10)), (5, (14, 10)), (5, (15, 10))]\n",
      "Purchases made by clients without debts: [(2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15)]\n"
     ]
    }
   ],
   "source": [
    "debts = [(1,3),(5,10),(10,2)]\n",
    "unpaid_cucumbers = spark.sparkContext.parallelize(debts)\n",
    "\n",
    "comp_registry = cucumbaLTDA_registry.join(unpaid_cucumbers)\n",
    "print(f'The complete registry obtained by the join of our sales registry and our debts is:\\n {comp_registry.collect()}')\n",
    "\n",
    "purchase_by_no_debt = cucumbaLTDA_registry.subtractByKey(unpaid_cucumbers)\n",
    "print(f'Purchases made by clients without debts: {purchase_by_no_debt.collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d3be1-d376-46be-a1f3-96ea739c157b",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9148cf-229b-412f-83c3-ebe0c39e38ef",
   "metadata": {},
   "source": [
    "**DataFrames** are the data structure that we will focus on in this course. Some bullet points that are of great importance about **DataFrames**:\n",
    "\n",
    "* Tabular structure. Excel-like data ☜(ﾟヮﾟ☜)\n",
    "* Immutable\n",
    "* Possess know schemas\n",
    "* Preserved linage. Transformations are saved step by step\n",
    "* Columns may have different d-types\n",
    "* Common methods such as **group by**, **order by** and **filter**\n",
    "* ***Extremely optimized on Spark***\n",
    "\n",
    "Without further adieu, let's create a **DataFrame** and play with it. As you may see, to correctly create a **DF** one need to pass as arguments the required data and its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde3aa4e-6c81-4ff1-9afd-a2b26dbdf5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 15|\n",
      "| Mary| 14|\n",
      "|James| 12|\n",
      "+-----+---+\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 15|\n",
      "+----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3 = [('John', 15),('Mary', 14),('James', 12)]\n",
    "\n",
    "schema = \"Name STRING, Age INT\" #Model of a schema accepted by spark\n",
    "\n",
    "df = spark.createDataFrame(data3, schema) #Here we pass the data and its schema\n",
    "\n",
    "df.show()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c62106b-2bd4-4861-ab9b-6885f6c735dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|   Product|Quantity|\n",
      "+----------+--------+\n",
      "|       Pen|       9|\n",
      "|Pineappple|      22|\n",
      "|     Apple|      12|\n",
      "|       Pen|      13|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema2 = \"Product STRING, Quantity INT\"\n",
    "data4 = [('Pen', 9),('Pineappple', 22),('Apple', 12),('Pen', 13)]\n",
    "\n",
    "df2 = spark.createDataFrame(data4, schema2)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48ed4a-256c-437f-a5a4-3963d82207b5",
   "metadata": {},
   "source": [
    "To get useful information about our **DF**, one can call particular **DF's attributes**. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37c6ed8-cf0e-426e-bd58-96de9dea9f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2's schema:StructType(List(StructField(Product,StringType,true),StructField(Quantity,IntegerType,true)))\n",
      "df2's columns:['Product', 'Quantity']\n"
     ]
    }
   ],
   "source": [
    "print(f'df2\\'s schema:{df2.schema}') ##Will return the previously described schema\n",
    "\n",
    "print(f'df2\\'s columns:{df2.columns}') ##Will return those previously described columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc2f53-a049-4e44-9a01-dcb3c676bc59",
   "metadata": {},
   "source": [
    "The new **DF** contains data from purchased products. One may need to see to total sum of a particular product, for instance. With this intention, we will use the method **groupBy** and **agg** (*agregate*). See below for a example on **agg** method for sumation and mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38ff34f-1c3d-4c9f-b879-0719295549d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|   Product|sum(Quantity)|\n",
      "+----------+-------------+\n",
      "|       Pen|           22|\n",
      "|     Apple|           12|\n",
      "|Pineappple|           22|\n",
      "+----------+-------------+\n",
      "\n",
      "+----------+-------------+\n",
      "|   Product|avg(Quantity)|\n",
      "+----------+-------------+\n",
      "|       Pen|         11.0|\n",
      "|     Apple|         12.0|\n",
      "|Pineappple|         22.0|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, mean\n",
    "df2.groupBy(\"Product\").agg(sum(\"Quantity\")).show()\n",
    "df2.groupBy(\"Product\").agg(mean(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccfc47-59b6-45eb-96d7-28a3b8778a6a",
   "metadata": {},
   "source": [
    "We can use the method **select** to choose different columns, or even add a particular expression for a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fefe6a2-bbb0-48d3-b15f-59310ea297d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+----------------+\n",
      "|   Product|Quantity|(Quantity - 4)|(Quantity * 0.5)|\n",
      "+----------+--------+--------------+----------------+\n",
      "|       Pen|       9|             5|             4.5|\n",
      "|Pineappple|      22|            18|            11.0|\n",
      "|     Apple|      12|             8|             6.0|\n",
      "|       Pen|      13|             9|             6.5|\n",
      "+----------+--------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df2.select(\"Product\", \"Quantity\", expr(\"Quantity - 4\"), expr(\"Quantity * 0.5\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43d4f9-2015-4b95-a05f-ac255f36b6e3",
   "metadata": {},
   "source": [
    "It is easy to see that the module **pyspark.sql.functions** will be widely used throughout this course. For this reason, will be useful to know our available functions. See ***https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html*** for further reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c012eba-c771-4228-aff6-74f7a68993a1",
   "metadata": {},
   "source": [
    "### DataFrame Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23254178-7db6-43f7-bfa7-96cd30cf9989",
   "metadata": {},
   "source": [
    "Let us take a look at an example file. In the folder ***Files***, there is a ***csv*** called **despachantes**. It contains data regarding sales made by sellers in a particular city and dates.\n",
    "\n",
    "Firstly, we must load this file into our environment. To do so, we can use a specific **spark.read.csv**, or a generic **spark.read.load**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97599ef8-eb01-47e3-b139-bcaf78be45d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load file with determined schema:\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "| id|               name|status|         city|sales|      date|\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05|\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "\n",
      "df shcema: StructType(List(StructField(id,IntegerType,true),StructField(name,StringType,true),StructField(status,StringType,true),StructField(city,StringType,true),StructField(sales,IntegerType,true),StructField(date,StringType,true)))\n",
      "\n",
      "Load file with infered schema:\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "\n",
      "df shcema: StructType(List(StructField(_c0,IntegerType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true),StructField(_c4,IntegerType,true),StructField(_c5,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "arqschema = \"id INT, name STRING, status STRING, city STRING, sales INT, date STRING\"\n",
    "\n",
    "dispatchers = spark.read.csv('Files/despachantes.csv', header = False, schema = arqschema)\n",
    "\n",
    "print('Load file with determined schema:')\n",
    "dispatchers.show()\n",
    "print(f'df shcema: {dispatchers.schema}\\n')\n",
    "\n",
    "dispatchers_autoschema = spark.read.load('Files/despachantes.csv', header = False, format = 'csv', sep = ',', inferSchema = True)\n",
    "\n",
    "print('Load file with infered schema:')\n",
    "dispatchers_autoschema.show()\n",
    "print(f'df shcema: {dispatchers_autoschema.schema}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61bd68-79a1-47c1-a100-aa05d342e46d",
   "metadata": {},
   "source": [
    "Remember, we can filter data or apply any funtion from the module **pyspark.sql.functions**. For example, let us create a filtered **DF** where the amount of sales are greater than 20. For more complex filters, the logic operators **& (AND)** and **~ (OR)** are present in **Spark**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39afb1f2-5de6-4b83-82cf-0af9f4d5ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+\n",
      "| id|               name|sales|\n",
      "+---+-------------------+-----+\n",
      "|  1|   Carminda Pestana|   23|\n",
      "|  2|    Deolinda Vilela|   34|\n",
      "|  3|   Emídio Dornelles|   34|\n",
      "|  4|Felisbela Dornelles|   36|\n",
      "|  6|   Matilde Rebouças|   22|\n",
      "|  7|    Noêmia   Orriça|   45|\n",
      "|  8|      Roque Vásquez|   65|\n",
      "|  9|      Uriel Queiroz|   54|\n",
      "+---+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dispatchers_goal = dispatchers.select('id', 'name', 'sales').where(col('sales') > 20) \n",
    "#It is always good to stress that this df has not been calculated.\n",
    "dispatchers_goal.show() #Only now, the calculation occurs! Lazy Evaluation ¯\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b1cd9-1aca-469e-aaed-e31a75d61a3c",
   "metadata": {},
   "source": [
    "Moreover, one may need to change a column name or a particular schema. Since **DF** are immutable objects from **Spark**, we must assing these changes as a new **DF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06bce4a5-e3db-4320-abed-26372b183f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n",
      "df shcema: StructType(List(StructField(id,IntegerType,true),StructField(names,StringType,true),StructField(status,StringType,true),StructField(city,StringType,true),StructField(sales,IntegerType,true),StructField(date,TimestampType,true)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "dispatchers_v2 = dispatchers.withColumnRenamed('name', 'names').withColumn('date', to_timestamp(col('date'), 'yyyy-MM-dd'))\n",
    "dispatchers_v2.show()\n",
    "print(f'df shcema: {dispatchers_v2.schema}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9743067-87a4-47f7-ac38-30a963c044c6",
   "metadata": {},
   "source": [
    "So far, we've only used the **show** action. Now we will see some of the available actions from **Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "375ff163-d8e6-4535-8756-5efc0bd3ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two rows from dispatchers_v2:\n",
      " [Row(id=1, names='Carminda Pestana', status='Ativo', city='Santa Maria', sales=23, date=datetime.datetime(2020, 8, 11, 0, 0)), Row(id=2, names='Deolinda Vilela', status='Ativo', city='Novo Hamburgo', sales=34, date=datetime.datetime(2020, 3, 5, 0, 0))]\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "Every row from dispatchers_v2 in the form of a list:\n",
      " [Row(id=1, names='Carminda Pestana', status='Ativo', city='Santa Maria', sales=23, date=datetime.datetime(2020, 8, 11, 0, 0)), Row(id=2, names='Deolinda Vilela', status='Ativo', city='Novo Hamburgo', sales=34, date=datetime.datetime(2020, 3, 5, 0, 0)), Row(id=3, names='Emídio Dornelles', status='Ativo', city='Porto Alegre', sales=34, date=datetime.datetime(2020, 2, 5, 0, 0)), Row(id=4, names='Felisbela Dornelles', status='Ativo', city='Porto Alegre', sales=36, date=datetime.datetime(2020, 2, 5, 0, 0)), Row(id=5, names='Graça Ornellas', status='Ativo', city='Porto Alegre', sales=12, date=datetime.datetime(2020, 2, 5, 0, 0)), Row(id=6, names='Matilde Rebouças', status='Ativo', city='Porto Alegre', sales=22, date=datetime.datetime(2019, 1, 5, 0, 0)), Row(id=7, names='Noêmia   Orriça', status='Ativo', city='Santa Maria', sales=45, date=datetime.datetime(2019, 10, 5, 0, 0)), Row(id=8, names='Roque Vásquez', status='Ativo', city='Porto Alegre', sales=65, date=datetime.datetime(2020, 3, 5, 0, 0)), Row(id=9, names='Uriel Queiroz', status='Ativo', city='Porto Alegre', sales=54, date=datetime.datetime(2018, 5, 5, 0, 0)), Row(id=10, names='Viviana Sequeira', status='Ativo', city='Porto Alegre', sales=0, date=datetime.datetime(2020, 9, 5, 0, 0))]\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "Count the number of rows: 10\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n"
     ]
    }
   ],
   "source": [
    "#Take a number of rows, and returns a list of them\n",
    "kool_log_separator = '-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-'\n",
    "print(f'First two rows from dispatchers_v2:\\n {dispatchers_v2.take(2)}') \n",
    "\n",
    "print(kool_log_separator)\n",
    "\n",
    "#Collect all rows in a list manner\n",
    "print(f'\\nEvery row from dispatchers_v2 in the form of a list:\\n {dispatchers_v2.collect()}')\n",
    "\n",
    "print(kool_log_separator)\n",
    "\n",
    "print(f'\\nCount the number of rows: {dispatchers_v2.count()}')\n",
    "\n",
    "print(kool_log_separator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66aa22f-021b-4211-bf45-95fe79008958",
   "metadata": {},
   "source": [
    "It is easy to see that transformations with **PySpark** is kind of clunky. For this reason, it is better to load our data with **Spark**, but transform after loading with **SQL scripts**. This is one of the main ideas from the **ELT** proccess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1006cc-9d9f-4d3e-b45f-823256d8a624",
   "metadata": {},
   "source": [
    "Finnaly, we will write our new altered **DF**, **dispatchers_v2**, as a **.parquet** file. \n",
    "\n",
    "*P.S.: You can give different formats for the write action*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5d77ffd-2e7b-4d4d-9a9b-58a638277e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to save data because of an AnalysisException.\n",
      "File already saved as part-00000-67e3502e-f992-4fbb-9140-ebd65f52c1de-c000.snappy.parquet\n",
      "Unable to save data because of an AnalysisException.\n",
      "File already saved as part-00000-1a93a3af-8e27-490f-afdd-c22f02c02418-c000.json\n",
      "File already saved as .ipynb_checkpoints\n",
      "Unable to save data because of an AnalysisException.\n",
      "File already saved as part-00000-99518b3d-8038-46ee-b085-d1af7568696d-c000.csv\n",
      "File already saved as .ipynb_checkpoints\n",
      "Unable to save data because of an AnalysisException.\n",
      "File already saved as part-00000-8c7fc7fe-8114-4521-a995-df592a40147d-c000.snappy.orc\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.utils\n",
    "import os, sys\n",
    "\n",
    "possible_formats = ['parquet', 'json', 'csv', 'orc']\n",
    "\n",
    "file = 'dispatchers_v2'\n",
    "\n",
    "for f in possible_formats:\n",
    "    path = f'Exports/{f}_{file}'\n",
    "    try:\n",
    "        dispatchers_v2.write.format(f).save(path)\n",
    "        dirr = os.listdir(path)\n",
    "        for d in dirr:\n",
    "            if '_SUCCESS' not in d and 'crc' not in d:\n",
    "                print(f'File saved as {d}')\n",
    "        \n",
    "    except pyspark.sql.utils.AnalysisException:\n",
    "        print('Unable to save data because of an AnalysisException.')\n",
    "        dirr = os.listdir(path)\n",
    "        for d in dirr:\n",
    "            if '_SUCCESS' not in d and 'crc' not in d:\n",
    "                print(f'File already saved as {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de307ea7-d34e-4777-bdcf-80e1bf840200",
   "metadata": {},
   "source": [
    "These file can be loaded back into our **Spark** environment using **read** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce126ef-f063-402c-ad26-a05fbb4084ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading part-00000-67e3502e-f992-4fbb-9140-ebd65f52c1de-c000.snappy.parquet\n",
      "Loading part-00000-1a93a3af-8e27-490f-afdd-c22f02c02418-c000.json\n",
      "Loading part-00000-99518b3d-8038-46ee-b085-d1af7568696d-c000.csv\n",
      "Loading part-00000-8c7fc7fe-8114-4521-a995-df592a40147d-c000.snappy.orc\n"
     ]
    }
   ],
   "source": [
    "random_formats_structure = {}\n",
    "\n",
    "for f in possible_formats:\n",
    "    path = f'Exports/{f}_{file}'\n",
    "    dirr = os.listdir(path)\n",
    "    for d in dirr:\n",
    "            if '_SUCCESS' not in d and 'crc' not in d and 'ipynb' not in d:\n",
    "                print(f'Loading {d}')\n",
    "                try:\n",
    "                    random_formats_structure[d] = spark.read.format(f).load(path + '/' + d)\n",
    "                except pyspark.sql.utils.AnalysisException:\n",
    "                    print('Unexpected Analysis Exception')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c09e652-966f-4ff1-aea3-1c290fafa7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-67e3502e-f992-4fbb-9140-ebd65f52c1de-c000.snappy.parquet\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "part-00000-1a93a3af-8e27-490f-afdd-c22f02c02418-c000.json\n",
      "+-------------+--------------------+---+-------------------+-----+------+\n",
      "|         city|                date| id|              names|sales|status|\n",
      "+-------------+--------------------+---+-------------------+-----+------+\n",
      "|  Santa Maria|2020-08-11T00:00:...|  1|   Carminda Pestana|   23| Ativo|\n",
      "|Novo Hamburgo|2020-03-05T00:00:...|  2|    Deolinda Vilela|   34| Ativo|\n",
      "| Porto Alegre|2020-02-05T00:00:...|  3|   Emídio Dornelles|   34| Ativo|\n",
      "| Porto Alegre|2020-02-05T00:00:...|  4|Felisbela Dornelles|   36| Ativo|\n",
      "| Porto Alegre|2020-02-05T00:00:...|  5|     Graça Ornellas|   12| Ativo|\n",
      "| Porto Alegre|2019-01-05T00:00:...|  6|   Matilde Rebouças|   22| Ativo|\n",
      "|  Santa Maria|2019-10-05T00:00:...|  7|    Noêmia   Orriça|   45| Ativo|\n",
      "| Porto Alegre|2020-03-05T00:00:...|  8|      Roque Vásquez|   65| Ativo|\n",
      "| Porto Alegre|2018-05-05T00:00:...|  9|      Uriel Queiroz|   54| Ativo|\n",
      "| Porto Alegre|2020-09-05T00:00:...| 10|   Viviana Sequeira|    0| Ativo|\n",
      "+-------------+--------------------+---+-------------------+-----+------+\n",
      "\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "part-00000-99518b3d-8038-46ee-b085-d1af7568696d-c000.csv\n",
      "+---+-------------------+-----+-------------+---+--------------------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|                 _c5|\n",
      "+---+-------------------+-----+-------------+---+--------------------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11T00:00:...|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05T00:00:...|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05T00:00:...|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05T00:00:...|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05T00:00:...|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05T00:00:...|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05T00:00:...|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05T00:00:...|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05T00:00:...|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05T00:00:...|\n",
      "+---+-------------------+-----+-------------+---+--------------------+\n",
      "\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "part-00000-8c7fc7fe-8114-4521-a995-df592a40147d-c000.snappy.orc\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for structure in random_formats_structure:\n",
    "    print(structure)\n",
    "    random_formats_structure[structure].show()\n",
    "    print(kool_log_separator + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecce2f8-79c4-4ec9-832a-99c17700ac29",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0897fa5-938a-4e90-8dab-8425801186b2",
   "metadata": {},
   "source": [
    "On the folder **Atividades** you will find five **parquet** files. You are required to explore the data, and deliver queries with the following products:\n",
    "\n",
    "* A query that shows three columns - Name, State and Status - with its respective data. They need to be in this particular order;\n",
    "\n",
    "* A second query that shows only clients with **platinum** and **gold** Status;\n",
    "\n",
    "* Calculate the percentage of sales regarding each Status.\n",
    "\n",
    "It is not absolute that you need to use every file. You can see the relation between them in the table scheme below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a53e0d-6f95-4fcd-b1eb-527121a72c0b",
   "metadata": {},
   "source": [
    "![Table Scheme](images/activity_schema.png \"Table Scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a96a64-fa98-4152-80a2-74ac49210fd7",
   "metadata": {},
   "source": [
    "First of all, we need to import all required files. We can see which files we will need by analyzing the table schema above.\n",
    "\n",
    "For the first question, we need the columns **Customer**, **Status**, and **Status**, therefore, we must load the data from the **Customers** table. We need only a filter of the previously created table for the second assignment. For the last assignment, we need to count the number of sales grouped by **Status**, hence, we must also load the **Sales** table.\n",
    "\n",
    "Considering everything, we can load only two tables from our schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917c7217-0ff0-487a-925c-db968ad4ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = spark.read.format('parquet').load('Files/Atividades/Clientes.parquet')\n",
    "\n",
    "sales = spark.read.format('parquet').load('Files/Atividades/Vendas.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be06b561-d3d5-4ba5-a8bd-976df35adbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------+--------+\n",
      "|ClienteID|             Cliente|Estado|Genero|  Status|\n",
      "+---------+--------------------+------+------+--------+\n",
      "|        1|Adelina Buenaventura|    RJ|     M|  Silver|\n",
      "|        2|        Adelino Gago|    RJ|     M|  Silver|\n",
      "|        3|     Adolfo Patrício|    PE|     M|  Silver|\n",
      "|        4|    Adriana Guedelha|    RO|     F|Platinum|\n",
      "|        5|       Adélio Lisboa|    SE|     M|  Silver|\n",
      "|        6|       Adérito Bahía|    MA|     M|  Silver|\n",
      "|        7|       Aida Dorneles|    RN|     F|  Silver|\n",
      "|        8|   Alarico Quinterno|    AC|     M|  Silver|\n",
      "|        9|    Alberto Cezimbra|    AM|     M|  Silver|\n",
      "|       10|    Alberto Monsanto|    RN|     M|    Gold|\n",
      "|       11|       Albino Canela|    AC|     M|  Silver|\n",
      "|       12|     Alceste Varanda|    RR|     F|  Silver|\n",
      "|       13|  Alcides Carvalhais|    RO|     M|  Silver|\n",
      "|       14|        Aldo Martins|    GO|     M|  Silver|\n",
      "|       15|   Alexandra Tabares|    MG|     F|  Silver|\n",
      "|       16|      Alfredo Cotrim|    SC|     M|  Silver|\n",
      "|       17|     Almeno Figueira|    SC|     M|  Silver|\n",
      "|       18|      Alvito Peralta|    AM|     M|  Silver|\n",
      "|       19|     Amadeu Martinho|    RN|     M|  Silver|\n",
      "|       20|      Amélia Estévez|    PE|     F|  Silver|\n",
      "+---------+--------------------+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "136bbe50-1bd6-4da9-a54c-775dea2b299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+---------+--------+\n",
      "|VendasID|VendedorID|ClienteID|     Data|   Total|\n",
      "+--------+----------+---------+---------+--------+\n",
      "|       1|         1|       91| 1/1/2019|  8053.6|\n",
      "|       2|         6|      185| 1/1/2020|   150.4|\n",
      "|       3|         7|       31| 2/1/2020|  6087.0|\n",
      "|       4|         5|       31| 2/1/2019| 13828.6|\n",
      "|       5|         5|       31| 3/1/2018|26096.66|\n",
      "|       6|         5|       31| 4/1/2020| 18402.0|\n",
      "|       7|         5|       31| 6/1/2019|  7524.2|\n",
      "|       8|         5|      186| 6/1/2019| 12036.6|\n",
      "|       9|         7|       91| 6/1/2020| 2804.75|\n",
      "|      10|         2|      202| 6/1/2020|  8852.0|\n",
      "|      11|         7|       58| 8/1/2019|16545.25|\n",
      "|      12|         7|       58| 9/1/2018|11411.88|\n",
      "|      13|         7|       58|10/1/2019| 15829.7|\n",
      "|      14|         3|      249|12/1/2020| 6154.36|\n",
      "|      15|         4|      249|12/1/2018| 3255.08|\n",
      "|      16|         7|      192|13/1/2020| 2901.25|\n",
      "|      17|         2|       79|13/1/2019| 15829.7|\n",
      "|      18|        10|       79|14/1/2019|16996.36|\n",
      "|      19|        10|      191|14/1/2019|   155.0|\n",
      "|      20|         9|      218|15/1/2018|  131.75|\n",
      "+--------+----------+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3abe11-a82b-4cf5-b42d-35d6adb9982f",
   "metadata": {},
   "source": [
    "For the first requirement, we shall use a simple **select** to get a few columns from **clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55351660-937d-48df-bfc4-31ca47a9d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n",
      "|                Name|State|  Status|\n",
      "+--------------------+-----+--------+\n",
      "|Adelina Buenaventura|   RJ|  Silver|\n",
      "|        Adelino Gago|   RJ|  Silver|\n",
      "|     Adolfo Patrício|   PE|  Silver|\n",
      "|    Adriana Guedelha|   RO|Platinum|\n",
      "|       Adélio Lisboa|   SE|  Silver|\n",
      "|       Adérito Bahía|   MA|  Silver|\n",
      "|       Aida Dorneles|   RN|  Silver|\n",
      "|   Alarico Quinterno|   AC|  Silver|\n",
      "|    Alberto Cezimbra|   AM|  Silver|\n",
      "|    Alberto Monsanto|   RN|    Gold|\n",
      "|       Albino Canela|   AC|  Silver|\n",
      "|     Alceste Varanda|   RR|  Silver|\n",
      "|  Alcides Carvalhais|   RO|  Silver|\n",
      "|        Aldo Martins|   GO|  Silver|\n",
      "|   Alexandra Tabares|   MG|  Silver|\n",
      "|      Alfredo Cotrim|   SC|  Silver|\n",
      "|     Almeno Figueira|   SC|  Silver|\n",
      "|      Alvito Peralta|   AM|  Silver|\n",
      "|     Amadeu Martinho|   RN|  Silver|\n",
      "|      Amélia Estévez|   PE|  Silver|\n",
      "+--------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df = clients.withColumnRenamed('Cliente', 'Name').withColumnRenamed('Estado', 'State').select('Name', 'State', 'Status')\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c54cc-fe9f-4549-aed1-3f87013e5968",
   "metadata": {},
   "source": [
    "Moreover, we were asked to filter the new **small_df** by **Status**, where **Status** equals **Gold** and **Platinum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5404d1ab-dd7b-4c8c-96c2-510382770bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+\n",
      "|               Name|State|  Status|\n",
      "+-------------------+-----+--------+\n",
      "|   Adriana Guedelha|   RO|Platinum|\n",
      "|   Alberto Monsanto|   RN|    Gold|\n",
      "|      Anna Carvajal|   RS|    Gold|\n",
      "|      Bento Quintão|   SP|    Gold|\n",
      "|      Carminda Dias|   AM|    Gold|\n",
      "|      Cláudio Jorge|   TO|    Gold|\n",
      "|    Dionísio Saltão|   PR|    Gold|\n",
      "|   Firmino Meireles|   AM|    Gold|\n",
      "|      Flor Vilanova|   CE|Platinum|\n",
      "|Honorina Villaverde|   PE|    Gold|\n",
      "|    Ibijara Botelho|   RR|Platinum|\n",
      "|  Iracema Rodríguez|   BA|    Gold|\n",
      "|         Joana Ataí|   GO|Platinum|\n",
      "+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "required_status = ['Gold', 'Platinum']\n",
    "\n",
    "filtered_small_df = small_df.filter(small_df.Status.isin(required_status))\n",
    "## As an alternative, you could use \n",
    "#select(\"*\").where((Func.col(\"Status\") == \"Gold\") |(Func.col(\"Status\") == \"Platinum\"))\n",
    "filtered_small_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656523b7-f9fb-424d-a3b3-3d7faeb528c9",
   "metadata": {},
   "source": [
    "The last assingment require us to count the number of sales grouped by each **Status**. We begin by joining our **clients** table with **sales** by the key **ClienteID**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6106c0b-9dc0-4c37-875d-b9185d90b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|  Status|sum(Total)|\n",
      "+--------+----------+\n",
      "|    Gold|  27286.69|\n",
      "|Platinum|  12584.68|\n",
      "|  Silver|3014291.36|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clients.join(sales, clients.ClienteID == sales.ClienteID, \"left\").select('Status', 'Total').groupby('Status').agg(sum('Total')).orderBy('Status').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71908b10-b8f9-49a7-b7bc-d82b53bd2bee",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a20fca-8934-4044-90ea-9ff96ae68f40",
   "metadata": {},
   "source": [
    "**Spark SQL** uses **Metastore** from **Hive** to create databases, which will enable us to process high-level languages, such as a **Structured Query Language**. For this reason, **Spark SQL** will search data from tables on a database. These tables are persistent objects, i.e., they will remain on our database after the ending of our last **Spark Session**.\n",
    "\n",
    "We must stress that **Tables** and **DataFrames** are interchangeable objects! One can easily create one from another.\n",
    "\n",
    "Another useful entity from **Spark SQL** is **Views**. It is an object that does not store data, just an alias for a complex query.\n",
    "\n",
    "Without delay, let us create a **Database** and a **Table** using **Spark SQL**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8651759-1532-4c32-9a6c-ce13ab6de6cf",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38c2be3b-4a1a-4823-a3fc-df61b37c2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     desp|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "#     Both import above are required to create a Database and a Table. Firstly, we begin the SparkSession, \n",
    "#   then we we use different types to create and operate on our Database\n",
    "\n",
    "spark.sql(\"show databases\").show() #This command help us see actives Databases\n",
    "\n",
    "#We can begin by creating a database (づ￣ ³￣)づ\n",
    "try:\n",
    "    spark.sql(\"create database desp\") #One can easily notice that argument of the sql type, we pass a sql script!\n",
    "except pyspark.sql.utils.AnalysisException:\n",
    "    print('Unexpected Analysis Exception. Database may already exist')   \n",
    "\n",
    "\n",
    "spark.sql(\"show databases\").show() #Checking if everything worked out well... ~(˘-˘~)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085fdf6-ae66-42ae-950b-4232ee90520a",
   "metadata": {},
   "source": [
    "We want to create a table on our new **Database**. Se we will run the command \"use (database)\" to change where our queries are pointed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f720b04a-9e36-41c9-9482-ac97b95c535a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use desp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a3cdd-692b-442f-a6c0-5bf30b9a2811",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b969515-0d4b-4b95-93d2-e41ae55fa073",
   "metadata": {},
   "source": [
    "Now we will create a table using a previously defined **DF**. It is easy to create a **Table** from a **DF**, we just need to have the **DF** active in our current **SparkSession**, then we write as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98c6e58e-fcea-4758-abc0-19b2f8069850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new table was added to the current database.\n"
     ]
    }
   ],
   "source": [
    "# A notebook related workaround that I'm using to signal AnalysisException\n",
    "try:\n",
    "    dispatchers.write.saveAsTable(\"Dispatchers\") \n",
    "    print('A new table was added to the current database.')\n",
    "except pyspark.sql.utils.AnalysisException:\n",
    "    print('Unexpected Analysis Exception. Table may already exist') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbcb712-b395-41ed-8bff-5ab5cfb702fe",
   "metadata": {},
   "source": [
    "Now we run a query to select data from our new table. Furthermore, we may use a command **show tables** to see every table from a active **Database**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2bc6ed-3531-44a1-935d-7dbab5205617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+-----+----------+\n",
      "| id|               name|status|         city|sales|      date|\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05|\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "\n",
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|     desp|dispatchers|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Dispatchers\").show()\n",
    "\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fdd80-2056-4de8-9174-76fc40e7b1f4",
   "metadata": {},
   "source": [
    "You can see above, that the **table** object is not temporary, i.e., after the end of our **SparkSession** the table will persist. \n",
    "\n",
    "The **table** we've just created is known as a **Manged Table**. This means that **Spark** manages both data and metadata, therefore, changes made to the table also happens physicaly on the stored data.\n",
    "\n",
    "Now, say we've made some changes on our **DF**, **dispatchers**, and we would like to **overwrite** the present **table** on our **Database**. For this, we simply add a **mode(\"overwrite\")** on the **write** command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31bcb5d6-0214-4f64-a282-d9af6ee6ea4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispatchers_v2.write.mode('overwrite').saveAsTable(\"Dispatchers\")\n",
    "\n",
    "spark.sql(\"select * from Dispatchers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2bce3-b52a-4d04-9de2-e7bd843ca83d",
   "metadata": {},
   "source": [
    "Next, we shall create an **External Table** (**Un-Managed**). This requires a path for a **parquet** file. This type of structure is not entirely managed by **Spark**, only the metadata are available for alteration. First, we write a new **parquet**, then we save it as a Table with a **option(\"path\", *absolute_path*)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d5e795a-b4a0-4c74-85d9-5cc4a58e5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatchers_v2.write.mode('overwrite').option(\"path\", \"/home/corbanez/Documents/PySpark/ExternalTables/dispatchers_v2_nm\").saveAsTable(\"Dispatchers_nm\") ##Path must be absolute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "395c4a9f-8087-4b08-b1e5-49a4d7240f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Dispatchers_nm\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a765ae28-6656-4388-82b7-47990922ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|   Product|Quantity|\n",
      "+----------+--------+\n",
      "|       Pen|       9|\n",
      "|Pineappple|      22|\n",
      "|     Apple|      12|\n",
      "|       Pen|      13|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.write.mode('overwrite').option(\"path\", \"/home/corbanez/Documents/PySpark/ExternalTables/ppap_nm\").saveAsTable(\"ppap_nm\") #Another example of external table!\n",
    "\n",
    "spark.sql(\"select * from ppap_nm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58fae6-7b2f-45e0-94a3-cd21406a5ebd",
   "metadata": {},
   "source": [
    "Now, if we run the command **show tables**, we should get two new **tables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04fb1472-69e6-43e6-bc94-d1fa67267a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|     desp|   dispatchers|      false|\n",
      "|     desp|dispatchers_nm|      false|\n",
      "|     desp|       ppap_nm|      false|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081e2e0-20c0-417a-8e77-a6cc57ea27dd",
   "metadata": {},
   "source": [
    "However, as previously discussed, there is a difference between them. The table **dispatchers** is a **Managed Table**, while the last two are **External Tables**. One might ask how do we differenciate them. The answer is easy, we just need to see the creation query. **Managed Tables** will not have a location parameter, whereas **External Tables** will have a fixed location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a791fb53-3e9a-4a25-8919-824d7eb70767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `desp`.`dispatchers` (\\n  `id` INT,\\n  `names` STRING,\\n  `status` STRING,\\n  `city` STRING,\\n  `sales` INT,\\n  `date` TIMESTAMP)\\nUSING parquet\\n|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `desp`.`dispatchers_nm` (\\n  `id` INT,\\n  `names` STRING,\\n  `status` STRING,\\n  `city` STRING,\\n  `sales` INT,\\n  `date` TIMESTAMP)\\nUSING parquet\\nLOCATION 'file:/home/corbanez/Documents/PySpark/ExternalTables/dispatchers_v2_nm'\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `desp`.`ppap_nm` (\\n  `Product` STRING,\\n  `Quantity` INT)\\nUSING parquet\\nLOCATION 'file:/home/corbanez/Documents/PySpark/ExternalTables/ppap_nm'\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table dispatchers\").show(truncate = False) ##Truncate = False enables the printing of the whole string\n",
    "print(kool_log_separator + '\\n')\n",
    "spark.sql(\"show create table dispatchers_nm\").show(truncate = False)\n",
    "print(kool_log_separator + '\\n')\n",
    "spark.sql(\"show create table ppap_nm\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68209c53-bc1a-409c-a5b9-09e3cbced29a",
   "metadata": {},
   "source": [
    "Another way to get metadata from these tables is with the command **spark.catalog.listTables()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03c97d0c-593d-43e9-965c-9b0979d67129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dispatchers', database='desp', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='dispatchers_nm', database='desp', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='ppap_nm', database='desp', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b488c8-c2fb-4a8f-a274-38fea01ac57a",
   "metadata": {},
   "source": [
    "## Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aad981-045c-494a-9858-abc6de19426d",
   "metadata": {},
   "source": [
    "In **SQL**, a **View** is a virtual **table** based on the result-set of a query. The fields in a view are fields from one or more real **tables** in the **database**. On **PySpark** one can create a **View** using the command **DataFrame.createOrReplaceTempView(*view_name*)** -for temporary **Views**- or, **DataFramecreateOrReplaceGlobalTempView(*view_name*)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e889eb8-4b4a-426c-b244-271c6577e513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+-----+----------+\n",
      "| id|               name|status|         city|sales|      date|\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05|\n",
      "+---+-------------------+------+-------------+-----+----------+\n",
      "\n",
      "+---------+--------------+-----------+\n",
      "|namespace|      viewName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|         |vw_dispatchers|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n",
      "+----------+--------+\n",
      "|   Product|Quantity|\n",
      "+----------+--------+\n",
      "|       Pen|       9|\n",
      "|Pineappple|      22|\n",
      "|     Apple|      12|\n",
      "|       Pen|      13|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispatchers.createOrReplaceTempView(\"vw_dispatchers\")\n",
    "\n",
    "spark.sql(\"select * from vw_dispatchers\").show() #Select of the view we've created\n",
    "\n",
    "spark.sql(\"show views\").show() #Command to get all temporary views from our database\n",
    "\n",
    "df2.createOrReplaceGlobalTempView(\"vw_ppap_nm\")\n",
    "\n",
    "spark.sql(\"select * from global_temp.vw_ppap_nm\").show() #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a9d30-b0ad-4a78-b9ae-a8b6f43f37f2",
   "metadata": {},
   "source": [
    "We can also create **Views** using regular **SQL**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bfbd5f9-96a8-4326-ac47-d9351ec07785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+-----------------+-----------+\n",
      "|namespace|         viewName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|         |   vw_dispatchers|       true|\n",
      "|         |vw_dispatchers_v2|       true|\n",
      "+---------+-----------------+-----------+\n",
      "\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create or replace temp view vw_dispatchers_v2 as select * from dispatchers_nm\").show()\n",
    "\n",
    "spark.sql(\"show views\").show()\n",
    "\n",
    "spark.sql(\"select * from vw_dispatchers_v2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae95679-714a-41ba-9da3-554aa516bd8f",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06bf08-ba7f-4cbf-891b-3e45535e3b65",
   "metadata": {},
   "source": [
    "Given that almost all modern **Databases** are normalized, one of the most used **SQL Clause** are **Joins**. **PySpark** has this transformation coded in the API, and is possible to make **Joins** with **PySpark SQL** as well. Let us take a deeper look into how we can make **Joins**, and all possible ways to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba72bf-6903-434d-92ca-182e94f67e95",
   "metadata": {},
   "source": [
    "We have been using data from the file **despachantes.csv**. This data is related by a key to the file **reclamacoes.csv**. See the schema bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf1de8-a729-47d4-8239-a9fa952e7146",
   "metadata": {},
   "source": [
    "![Table Scheme](images/join_example.png \"Table Scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28deafd1-7ae3-4826-b20d-ca6d3c3e44b6",
   "metadata": {},
   "source": [
    "We begin by loading the data from this new table into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "107bb622-3387-4251-aec7-fc658bdb8e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+\n",
      "|id_c|    date_c|id_dispatcher|\n",
      "+----+----------+-------------+\n",
      "|   1|2020-09-12|            2|\n",
      "|   2|2020-09-11|            2|\n",
      "|   3|2020-10-05|            4|\n",
      "|   4|2020-10-02|            5|\n",
      "|   5|2020-12-06|            5|\n",
      "|   6|2020-01-09|            5|\n",
      "|   7|2020-01-05|            9|\n",
      "+----+----------+-------------+\n",
      "\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "| id|              names|status|         city|sales|               date|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|   23|2020-08-11 00:00:00|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|   34|2020-03-05 00:00:00|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|   34|2020-02-05 00:00:00|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|   36|2020-02-05 00:00:00|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|   12|2020-02-05 00:00:00|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|   22|2019-01-05 00:00:00|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|   45|2019-10-05 00:00:00|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|   65|2020-03-05 00:00:00|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|   54|2018-05-05 00:00:00|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|    0|2020-09-05 00:00:00|\n",
      "+---+-------------------+------+-------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complains = spark.read.load('Files/reclamacoes.csv', header = False, format = 'csv', sep = ',', inferSchema = True)\n",
    "complains = complains.withColumnRenamed('_c0', 'id_c').withColumnRenamed('_c1', 'date_c').withColumnRenamed('_c2', 'id_dispatcher')\n",
    "\n",
    "complains.write.mode('overwrite').saveAsTable('Complains')\n",
    "\n",
    "spark.sql('select * from Complains').show()\n",
    "\n",
    "spark.sql('select * from Dispatchers').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfee540-ab2b-4220-84f7-115ba22a0676",
   "metadata": {},
   "source": [
    "With both **tables** present on our **Database** we can perform **Join** operarions as we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a177a788-169e-4493-bef0-f91675f1526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+-------------------+\n",
      "|id_c|    date_c|id_dispatcher|              names|\n",
      "+----+----------+-------------+-------------------+\n",
      "|   1|2020-09-12|            2|    Deolinda Vilela|\n",
      "|   2|2020-09-11|            2|    Deolinda Vilela|\n",
      "|   3|2020-10-05|            4|Felisbela Dornelles|\n",
      "|   4|2020-10-02|            5|     Graça Ornellas|\n",
      "|   5|2020-12-06|            5|     Graça Ornellas|\n",
      "|   6|2020-01-09|            5|     Graça Ornellas|\n",
      "|   7|2020-01-05|            9|      Uriel Queiroz|\n",
      "+----+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select complains.*, dispatchers.names from complains left join dispatchers on dispatchers.id = complains.id_dispatcher').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71877ee-ff6b-4722-b920-763057ac0c88",
   "metadata": {},
   "source": [
    "One could also use the **Spark API** to perform these **Joins** directly from **DFs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65402977-fcf4-4e90-a64c-07d2514aa78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+-------------------+\n",
      "|id_c|      date|id_dispatcher|               name|\n",
      "+----+----------+-------------+-------------------+\n",
      "|   1|2020-03-05|            2|    Deolinda Vilela|\n",
      "|   2|2020-03-05|            2|    Deolinda Vilela|\n",
      "|   3|2020-02-05|            4|Felisbela Dornelles|\n",
      "|   4|2020-02-05|            5|     Graça Ornellas|\n",
      "|   5|2020-02-05|            5|     Graça Ornellas|\n",
      "|   6|2020-02-05|            5|     Graça Ornellas|\n",
      "|   7|2018-05-05|            9|      Uriel Queiroz|\n",
      "+----+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complains.join(dispatchers, dispatchers.id == complains.id_dispatcher, \"left\").select('id_c', 'date', 'id_dispatcher', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f2ddc-34b3-470a-9048-acffb374f009",
   "metadata": {},
   "source": [
    "We can run two separate cells without the **.show()** action to analyze how performative each method is. Given the regular volume of modern **Databases**, the choice between **DFs** and **Tables** is very significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3f2ced-a36d-4024-a1d1-e24e28d1200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.82 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "##Using API. (༎ຶ Д༎ຶ`) WHY SOO UGLY?!\n",
    "complains.join(dispatchers, dispatchers.id == complains.id_dispatcher, \"left\").select('id_c', 'date', 'id_dispatcher', 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bdd6ea8-a255-416a-a7d4-d105cfdfc4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.46 ms ± 371 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "##Using SQL. (づ￣ ³￣)づ MUCH FAST! MUCH CLEAN!!\n",
    "spark.sql('select complains.*, dispatchers.names from complains left join dispatchers on dispatchers.id = complains.id_dispatcher')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a427c86-ab50-404c-b6c9-ccaabaedfc54",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a320e30-212a-453b-a114-61de3309589d",
   "metadata": {},
   "source": [
    "Given the same schema from the last activity, you must create a **Database** called **retail_sales**. Then, upload all data from the folder **Files/Atividades/** as individual **tables**. Finally, you should create a **SQL query** that returns the following columns: **Client Names**, **Sell Date**, **Product**, **Seller**, and **Value**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8db13f-1589-4b0b-8138-304e1806801e",
   "metadata": {},
   "source": [
    "![Table Scheme](images/activity_schema.png \"Table Scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa9ec1-48ee-4d5d-9402-4668feb65585",
   "metadata": {},
   "source": [
    "We begin by creating a **Database** with the complete data schema seen above. Firstly, we shall load all required data. (Remember that Sales and Clients data were already loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28b65f9e-e129-42f5-bb5d-efa22df8bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.format('parquet').load('Files/Atividades/Produtos.parquet')\n",
    "\n",
    "sallers = spark.read.format('parquet').load('Files/Atividades/Vendedores.parquet')\n",
    "\n",
    "itens_sales = spark.read.format('parquet').load('Files/Atividades/ItensVendas.parquet')\n",
    "\n",
    "#clients => Clientes.parquet\n",
    "#sales => Vendas.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc363a3-348b-4acc-9ccb-723b5f349404",
   "metadata": {},
   "source": [
    "Secondly, we will create a database **retail_sales** per requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cff20ad0-ae15-4763-8684-e127730833c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\"create database retail_sales\")\n",
    "except pyspark.sql.utils.AnalysisException: \n",
    "    print('Unexpected Analysis Exception. Database may already exist')  \n",
    "    \n",
    "##Remember to select the database you want to use!!\n",
    "\n",
    "spark.sql(\"use retail_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160f0b9-e8bd-4b14-8aac-95a00f818fa7",
   "metadata": {},
   "source": [
    "Finally, we write all **DFs** as tables in our newly created **Database**, **retail_sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22a94e12-c27d-4949-a212-ce97443e2b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-----------+\n",
      "|   namespace|        tableName|isTemporary|\n",
      "+------------+-----------------+-----------+\n",
      "|retail_sales|          clients|      false|\n",
      "|retail_sales|      itens_sales|      false|\n",
      "|retail_sales|         products|      false|\n",
      "|retail_sales|            sales|      false|\n",
      "|retail_sales|          sallers|      false|\n",
      "|            |   vw_dispatchers|       true|\n",
      "|            |vw_dispatchers_v2|       true|\n",
      "+------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.write.mode('overwrite').saveAsTable('Products')\n",
    "sallers.write.mode('overwrite').saveAsTable('Sallers')\n",
    "itens_sales.write.mode('overwrite').saveAsTable('itens_sales')\n",
    "sales.write.mode('overwrite').saveAsTable('Sales')\n",
    "clients.write.mode('overwrite').saveAsTable('Clients')\n",
    "\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "042820a5-7b46-4e86-b7e2-abb674c349d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------------------+----------------+\n",
      "|             Cliente|      Data|ValorTotal|             Produto|        Vendedor|\n",
      "+--------------------+----------+----------+--------------------+----------------+\n",
      "|Adelina Buenaventura|13/12/2019|    114.75|Camiseta Predacto...|Jéssica Castelão|\n",
      "|Adelina Buenaventura|13/12/2019|     103.5|Bermuda Predactor...|Jéssica Castelão|\n",
      "|Adelina Buenaventura|13/12/2019|   2268.99|Bicicleta Gometws...|Jéssica Castelão|\n",
      "|Adelina Buenaventura|13/12/2019|    6892.2|Bicicleta Trinc C...|Jéssica Castelão|\n",
      "|        Adelino Gago| 22/8/2020|    118.58|Capacete Gometws ...|   Daniel Pirajá|\n",
      "|        Adelino Gago| 22/8/2020|     188.0|Luva De Ciclismo ...|   Daniel Pirajá|\n",
      "|        Adelino Gago| 22/8/2020|    2521.1|Bicicleta Gometws...|   Daniel Pirajá|\n",
      "|        Adelino Gago| 22/8/2020|    2955.0|Bicicleta Gometws...|   Daniel Pirajá|\n",
      "|     Adolfo Patrício| 7/11/2020|     139.5|Capacete Gometws ...|  Hélio Liberato|\n",
      "|     Adolfo Patrício| 7/11/2020|    2009.4|Bicicleta Gometws...|  Hélio Liberato|\n",
      "|     Adolfo Patrício| 7/11/2020|    8510.0|Bicicleta Gts Adv...|  Hélio Liberato|\n",
      "|    Adriana Guedelha|      null|      null|                null|            null|\n",
      "|       Adélio Lisboa| 5/12/2019|    237.16|Capacete Gometws ...|    Armando Lago|\n",
      "|       Adélio Lisboa| 5/12/2019|    7658.0|Bicicleta Trinc C...|    Armando Lago|\n",
      "|       Adélio Lisboa| 5/12/2019|    9201.0|Bicicleta Altools...|    Armando Lago|\n",
      "|       Adélio Lisboa|23/11/2019|     139.5|Capacete Gometws ...|  Hélio Liberato|\n",
      "|       Adélio Lisboa|23/11/2019|     97.75|Bermuda Predactor...|  Hélio Liberato|\n",
      "|       Adélio Lisboa|23/11/2019|     159.8|Luva De Ciclismo ...|  Hélio Liberato|\n",
      "|       Adélio Lisboa|23/11/2019|    7081.6|Bicicleta Aro 29 ...|  Hélio Liberato|\n",
      "|       Adélio Lisboa|23/11/2019|   7820.85|Bicicleta Altools...|  Hélio Liberato|\n",
      "+--------------------+----------+----------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##It seems reasonable to create a sql script in a different IDE, and load the file into Python for a more concise code\n",
    "\n",
    "spark.sql('select clients.Cliente, sales.Data, itens_sales.ValorTotal, products.Produto, sallers.Vendedor from clients left join sales on sales.ClienteID = clients.ClienteID left join itens_sales on sales.VendasID = itens_sales.VendasID left join products on itens_sales.ProdutoID = products.ProdutoID left join sallers on sallers.VendedorID = sales.VendedorID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652be96-8c58-4e2d-850b-1ee5b6b6d518",
   "metadata": {},
   "source": [
    "# Exploring Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbaf5e-23ab-4242-a650-f35247c8d7c5",
   "metadata": {},
   "source": [
    "One can also import data from different data sources. Not only import, but write them as **DFs**, **Tables** and reload them into the original data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810469b-c3ce-4053-b4c7-61980c3156c5",
   "metadata": {},
   "source": [
    "## PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2aa7eb-821f-424f-b41f-48514f8caf6a",
   "metadata": {},
   "source": [
    "You should install **PostgreSQL** in your machine for this example. Run the command **sudo apt-get install postgresql-12**. Furthermore, you will need a **PostgreSQL Database** already setup. For further reading, follow the article: ***https://www.digitalocean.com/community/tutorials/how-to-install-and-use-postgresql-on-ubuntu-18-04-pt***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90134b23-ffe9-43d0-bde0-da4aec1f1e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
